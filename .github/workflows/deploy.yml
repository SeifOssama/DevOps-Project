name: Deploy Infrastructure & Services

on:
  workflow_dispatch:  # Manual trigger
  # Uncomment after testing to enable on push
  # push:
  #   branches:
  #     - main

env:
  AWS_REGION: us-east-1
  TERRAFORM_VERSION: 1.6.0
  ANSIBLE_VERSION: 2.15.0

jobs:
  deploy:
    runs-on: ubuntu-latest
    name: Deploy Infrastructure & Services
    
    steps:
      # ============================================
      # PHASE 0: Setup & Pre-flight Checks
      # ============================================
      - name: üìã Phase 0 - Checkout Code
        uses: actions/checkout@v4
      
      - name: ‚úÖ Test - Verify Repository Structure
        run: |
          echo "::group::Repository Structure Check"
          echo "Checking required directories..."
          
          required_dirs=("Terraform" "Ansible" "Monitoring" "scripts")
          for dir in "${required_dirs[@]}"; do
            if [ -d "$dir" ]; then
              echo "‚úÖ Found: $dir"
            else
              echo "‚ùå Missing: $dir"
              exit 1
            fi
          done
          
          echo "‚úÖ Repository structure validated"
          echo "::endgroup::"
      
      - name: üîê Phase 0 - Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: ‚úÖ Test - Verify AWS Credentials
        run: |
          echo "::group::AWS Credentials Verification"
          echo "Testing AWS credentials..."
          
          CALLER_IDENTITY=$(aws sts get-caller-identity)
          ACCOUNT_ID=$(echo $CALLER_IDENTITY | jq -r '.Account')
          USER_ARN=$(echo $CALLER_IDENTITY | jq -r '.Arn')
          
          echo "‚úÖ AWS Account: $ACCOUNT_ID"
          echo "‚úÖ IAM Identity: $USER_ARN"
          echo "‚úÖ AWS Region: $AWS_REGION"
          echo "‚úÖ AWS credentials are valid"
          echo "::endgroup::"
      
      - name: üîë Phase 0 - Setup SSH Agent
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}
      
      - name: ‚úÖ Test - Verify SSH Key Loaded
        run: |
          echo "::group::SSH Key Verification"
          echo "Checking SSH agent..."
          
          KEY_COUNT=$(ssh-add -L | wc -l)
          if [ "$KEY_COUNT" -ge 1 ]; then
            echo "‚úÖ SSH key loaded successfully"
            echo "   Key fingerprint:"
            ssh-add -l | head -1
          else
            echo "‚ùå No SSH keys found in agent"
            exit 1
          fi
          
          # Configure SSH to not check host keys (for automation)
          mkdir -p ~/.ssh
          cat >> ~/.ssh/config <<EOF
          Host *
            StrictHostKeyChecking no
            UserKnownHostsFile=/dev/null
            LogLevel ERROR
          EOF
          
          echo "‚úÖ SSH configured for automation"
          echo "::endgroup::"
      
      # ============================================
      # PHASE 1: Infrastructure Provisioning
      # ============================================
      - name: üèóÔ∏è Phase 1 - Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false  # Required for output parsing
      
      - name: üóÑÔ∏è Phase 1 - Ensure Backend Infrastructure Exists
        run: |
          echo "::group::Backend Infrastructure Setup"
          
          # Get AWS Account ID
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          BUCKET_NAME="devops-project-terraform-state-$ACCOUNT_ID"
          TABLE_NAME="terraform-state-locks"
          
          echo "üìã Target backend resources:"
          echo "   S3 Bucket: $BUCKET_NAME"
          echo "   DynamoDB Table: $TABLE_NAME"
          echo ""
          
          # ============================================
          # Create S3 Bucket if it doesn't exist
          # ============================================
          echo "Checking S3 bucket..."
          if aws s3 ls "s3://$BUCKET_NAME" 2>/dev/null; then
            echo "‚úÖ S3 bucket already exists: $BUCKET_NAME"
          else
            echo "üì¶ Creating S3 bucket: $BUCKET_NAME"
            aws s3api create-bucket \
              --bucket "$BUCKET_NAME" \
              --region us-east-1
            
            # Enable versioning
            echo "   Enabling versioning..."
            aws s3api put-bucket-versioning \
              --bucket "$BUCKET_NAME" \
              --versioning-configuration Status=Enabled
            
            # Enable encryption
            echo "   Enabling encryption..."
            aws s3api put-bucket-encryption \
              --bucket "$BUCKET_NAME" \
              --server-side-encryption-configuration '{
                "Rules": [{
                  "ApplyServerSideEncryptionByDefault": {
                    "SSEAlgorithm": "AES256"
                  }
                }]
              }'
            
            # Block public access
            echo "   Blocking public access..."
            aws s3api put-public-access-block \
              --bucket "$BUCKET_NAME" \
              --public-access-block-configuration \
                "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true"
            
            # Add tags
            echo "   Adding tags..."
            aws s3api put-bucket-tagging \
              --bucket "$BUCKET_NAME" \
              --tagging 'TagSet=[
                {Key=Name,Value="Terraform State Bucket"},
                {Key=Environment,Value="Production"},
                {Key=ManagedBy,Value="GitHub Actions"}
              ]'
            
            echo "‚úÖ S3 bucket created successfully"
          fi
          
          # ============================================
          # Create DynamoDB Table if it doesn't exist
          # ============================================
          echo ""
          echo "Checking DynamoDB table..."
          if aws dynamodb describe-table --table-name "$TABLE_NAME" 2>/dev/null; then
            echo "‚úÖ DynamoDB table already exists: $TABLE_NAME"
          else
            echo "üìä Creating DynamoDB table: $TABLE_NAME"
            aws dynamodb create-table \
              --table-name "$TABLE_NAME" \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST \
              --tags \
                Key=Name,Value="Terraform State Lock Table" \
                Key=Environment,Value="Production" \
                Key=ManagedBy,Value="GitHub Actions"
            
            echo "   Waiting for table to be active..."
            aws dynamodb wait table-exists --table-name "$TABLE_NAME"
            
            echo "‚úÖ DynamoDB table created successfully"
          fi
          
          echo ""
          echo "‚úÖ Backend infrastructure is ready"
          echo "::endgroup::"
      
      - name: üîì Phase 1 - Clear Stuck Locks (if any)
        if: always()
        continue-on-error: true
        run: |
          echo "::group::Lock Cleanup"
          
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          BUCKET_NAME="devops-project-terraform-state-$ACCOUNT_ID"
          
          echo "Checking for stuck locks..."
          aws dynamodb delete-item \
            --table-name terraform-state-locks \
            --key "{\"LockID\": {\"S\": \"$BUCKET_NAME/devops-project/terraform.tfstate\"}}" \
            2>/dev/null || echo "No stuck locks found (or table doesn't exist yet)"
          
          echo "::endgroup::"
      
      - name: üèóÔ∏è Phase 1 - Terraform Init
        run: |
          echo "::group::Terraform Initialization"
          cd Terraform
          
          echo "Initializing Terraform with remote backend..."
          echo "Backend: s3://${{ secrets.TF_BACKEND_BUCKET }}"
          echo "Locking: DynamoDB table ${{ secrets.TF_BACKEND_DYNAMODB_TABLE }}"
          echo ""
          
          terraform init \
            -backend-config="bucket=${{ secrets.TF_BACKEND_BUCKET }}" \
            -backend-config="dynamodb_table=${{ secrets.TF_BACKEND_DYNAMODB_TABLE }}" \
            -backend-config="region=${{ secrets.TF_BACKEND_REGION }}" \
            -input=false
          
          echo "‚úÖ Terraform initialized successfully"
          echo "::endgroup::"
      
      - name: üèóÔ∏è Phase 1 - Terraform Validate
        run: |
          echo "::group::Terraform Validation"
          cd Terraform
          terraform validate
          echo "::endgroup::"
      
      - name: ‚úÖ Test - Terraform Validation Success
        run: |
          echo "::group::Terraform Validation Test"
          cd Terraform
          
          # Check format
          echo "Checking Terraform formatting..."
          terraform fmt -check -recursive || {
            echo "‚ö†Ô∏è  Warning: Terraform files are not formatted"
            echo "   Run 'terraform fmt -recursive' to fix"
          }
          
          # Validate again to confirm
          if terraform validate > /dev/null 2>&1; then
            echo "‚úÖ Terraform configuration is valid"
          else
            echo "‚ùå Terraform validation failed"
            exit 1
          fi
          echo "::endgroup::"
      
      - name: üîÑ Phase 1 - Refresh State (Detect Drift)
        run: |
          echo "::group::State Refresh"
          cd Terraform
          
          echo "Refreshing Terraform state to detect any manual changes or drift..."
          echo "This will update the state to match the real-world infrastructure."
          echo ""
          
          terraform apply -refresh-only \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -auto-approve || {
            echo "‚ö†Ô∏è  State refresh encountered issues"
            echo "   This may indicate:"
            echo "   - Resources were manually deleted from AWS"
            echo "   - References to non-existent resources in state"
            echo "   - Permission issues"
            echo ""
            echo "   Terraform will attempt to reconcile during plan/apply"
          }
          
          echo ""
          echo "‚úÖ State refresh complete"
          echo "::endgroup::"
      
      - name: üîç Phase 1.5 - Auto-Import Existing Resources
        continue-on-error: true
        run: |
          echo "::group::Checking for Existing Resources"
          cd Terraform
          
          echo "Checking for resources that exist in AWS but not in Terraform state..."
          echo "This prevents 'already exists' errors during terraform apply."
          echo ""
          
          # Function to check if resource is in state
          resource_in_state() {
            terraform state list 2>/dev/null | grep -q "^$1$"
          }
          
          # Function to import resource
          import_resource() {
            local resource_address=$1
            local resource_id=$2
            local resource_name=$3
            
            echo "‚Üí Checking $resource_name..."
            
            if resource_in_state "$resource_address"; then
              echo "  ‚úÖ Already in state: $resource_address"
            else
              echo "  üì• Importing: $resource_address ‚Üí $resource_id"
              if terraform import "$resource_address" "$resource_id" 2>&1 | grep -q "Import successful"; then
                echo "  ‚úÖ Imported successfully"
              else
                echo "  ‚ö†Ô∏è  Import failed (resource may not exist in AWS yet)"
              fi
            fi
          }
          
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          echo "Checking Key Resources..."
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          
          # Import EC2 Key Pair
          import_resource "aws_key_pair.deployer" "deployer-key" "EC2 Key Pair"
          
          # Import IAM Role for Control Node
          import_resource "module.controlnode.aws_iam_role.control_node_role" "monitoring-node-role" "IAM Role (Control Node)"
          
          # Import IAM Instance Profile for Control Node
          import_resource "module.controlnode.aws_iam_instance_profile.control_node_profile" "monitoring-node-profile" "IAM Instance Profile (Control Node)"
          
          # Import IAM Role Policy Attachment (if exists)
          if aws iam list-attached-role-policies --role-name monitoring-node-role 2>/dev/null | grep -q "AmazonSSMManagedInstanceCore"; then
            import_resource "module.controlnode.aws_iam_role_policy_attachment.ssm_policy" "monitoring-node-role/arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore" "IAM Policy Attachment (SSM)"
          fi
          
          echo ""
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          echo "Checking Worker Node Resources..."
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          
          # Check for worker node IAM resources (they may not exist yet)
          if aws iam get-role --role-name worker-node-role 2>/dev/null; then
            import_resource "module.workernode.aws_iam_role.worker_role" "worker-node-role" "IAM Role (Worker Node)"
          else
            echo "‚ÑπÔ∏è  Worker node IAM role doesn't exist yet (will be created)"
          fi
          
          if aws iam get-instance-profile --instance-profile-name worker-node-profile 2>/dev/null; then
            import_resource "module.workernode.aws_iam_instance_profile.worker_profile" "worker-node-profile" "IAM Instance Profile (Worker Node)"
          else
            echo "‚ÑπÔ∏è  Worker node instance profile doesn't exist yet (will be created)"
          fi
          
          echo ""
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          echo "‚úÖ Auto-Import Complete!"
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          echo ""
          echo "Current resources in state:"
          terraform state list || echo "No resources in state yet"
          echo ""
          echo "::endgroup::"
      
      - name: üèóÔ∏è Phase 1 - Terraform Plan
        run: |
          echo "::group::Terraform Plan"
          cd Terraform
          terraform plan -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" -out=tfplan
          echo "::endgroup::"
      
      - name: üèóÔ∏è Phase 1 - Terraform Apply
        run: |
          echo "::group::Terraform Apply"
          cd Terraform
          terraform apply -auto-approve tfplan
          echo "::endgroup::"
      
      - name: ‚úÖ Test - Terraform Outputs Sanity Check
        id: tf_outputs
        run: |
          echo "::group::Terraform Outputs Validation"
          cd Terraform
          
          # Run validation script
          bash ../scripts/validate-terraform-outputs.sh
          
          # Save outputs for later steps
          MONITORING_IP=$(terraform output -raw monitoring_node_public_ip)
          WEBSERVER_IPS=$(terraform output -json webserver_public_ips | jq -r '.[]' | tr '\n' ' ')
          
          echo "monitoring_ip=$MONITORING_IP" >> $GITHUB_OUTPUT
          echo "webserver_ips=$WEBSERVER_IPS" >> $GITHUB_OUTPUT
          
          echo "::endgroup::"
      
      - name: üìä Phase 1 - Save Terraform Outputs
        run: |
          echo "::group::Saving Terraform Outputs"
          cd Terraform
          
          terraform output -json > terraform-outputs.json
          
          echo "üìÑ Terraform Outputs:"
          cat terraform-outputs.json | jq '.'
          
          echo "::endgroup::"
      
      - name: üì§ Upload Terraform Outputs
        uses: actions/upload-artifact@v4
        with:
          name: terraform-outputs
          path: Terraform/terraform-outputs.json
          retention-days: 7
      
      # ============================================
      # PHASE 2: Ansible Environment Setup
      # ============================================
      - name: üöÄ Phase 2 - Ansible Environment Setup
        run: |
          echo "========================================="
          echo "Phase 2: Ansible Environment Setup"
          echo "========================================="
      
      - name: üîç Phase 2.1 - Verify Pre-installed Tools
        run: |
          echo "::group::Pre-installed Versions (Before Installation)"
          echo "=== Pre-installed Versions (Before Installation) ==="
          echo ""
          echo "Python: $(python3 --version 2>&1 || echo 'Not found')"
          echo "Pip: $(pip3 --version 2>&1 || echo 'Not found')"
          echo "AWS CLI: $(aws --version 2>&1 || echo 'Not found')"
          echo "Ansible (if pre-installed): $(ansible --version 2>&1 | head -n 1 || echo 'Not found')"
          echo ""
          echo "::endgroup::"
      
      - name: üì¶ Phase 2.2 - Install Python Dependencies
        run: |
          echo "::group::Installing Python Packages"
          echo "=== Installing Python Packages ==="
          
          # Upgrade pip first
          echo "Upgrading pip..."
          python3 -m pip install --upgrade pip
          
          # Install boto3 and botocore with specific version requirements
          echo ""
          echo "Installing boto3 and botocore..."
          python3 -m pip install 'boto3>=1.34.0' 'botocore>=1.34.0'
          
          # Install Ansible (this installs to system Python, NOT pipx)
          echo ""
          echo "Installing Ansible..."
          python3 -m pip install 'ansible>=2.17'
          
          echo ""
          echo "‚úÖ Python packages installed successfully"
          echo "::endgroup::"
      
      - name: üéØ Phase 2.3 - Install Ansible Collections
        run: |
          echo "::group::Installing Ansible Collections"
          echo "=== Installing Ansible Collections ==="
          
          # Install amazon.aws collection
          ansible-galaxy collection install amazon.aws
          
          echo ""
          echo "‚úÖ Ansible collections installed successfully"
          echo "::endgroup::"
      
      - name: ‚úÖ Phase 2.4 - Verify Complete Installation
        run: |
          echo "::group::Installation Verification"
          echo "========================================="
          echo "Installation Verification"
          echo "========================================="
          
          echo ""
          echo "=== Python Version ==="
          python3 --version
          
          echo ""
          echo "=== Pip Version ==="
          pip3 --version
          
          echo ""
          echo "=== Ansible Version and Python Interpreter ==="
          ansible --version
          
          echo ""
          echo "=== Boto3 Version ==="
          python3 -c "import boto3; print(f'boto3: {boto3.__version__}')"
          
          echo ""
          echo "=== Botocore Version ==="
          python3 -c "import botocore; print(f'botocore: {botocore.__version__}')"
          
          echo ""
          echo "=== AWS CLI Version ==="
          aws --version
          
          echo ""
          echo "=== Installed Ansible Collections ==="
          ansible-galaxy collection list amazon.aws
          
          echo ""
          echo "=== Test AWS EC2 Inventory Plugin Documentation ==="
          ansible-doc -t inventory amazon.aws.aws_ec2 | head -n 5
          
          echo ""
          echo "‚úÖ All installations verified successfully"
          echo "::endgroup::"
      
      - name: üîó Phase 2.5 - Test AWS Connectivity
        run: |
          echo "::group::AWS Connectivity Test"
          echo "=== Testing AWS Connectivity ==="
          
          echo "Testing AWS EC2 API access..."
          aws ec2 describe-instances --region ${{ env.AWS_REGION }} --query 'Reservations[*].Instances[*].[InstanceId,State.Name,Tags[?Key==`Name`].Value|[0]]' --output table || echo "No instances found or connection issue"
          
          echo ""
          echo "‚úÖ AWS connectivity verified"
          echo "::endgroup::"
      
      - name: ‚úÖ Test - Verify AWS EC2 Dynamic Inventory Plugin
        run: |
          echo "::group::AWS EC2 Plugin Verification"
          
          cd Ansible
          
          # Verify aws_ec2.yml exists
          if [ ! -f "inventory/aws_ec2.yml" ]; then
            echo "‚ùå inventory/aws_ec2.yml not found"
            exit 1
          fi
          
          echo "Testing AWS EC2 dynamic inventory plugin..."
          echo ""
          
          # Test 1: ansible-inventory --list (comprehensive inventory data)
          echo "1Ô∏è‚É£ Testing ansible-inventory --list..."
          if ansible-inventory -i inventory/aws_ec2.yml --list > /tmp/inventory_list.json; then
            echo "‚úÖ ansible-inventory --list executed successfully"
            
            # Display the full inventory JSON for debugging
            echo ""
            echo "üìã Full Inventory Output:"
            cat /tmp/inventory_list.json | jq '.'
            echo ""
            
            # Check if output contains aws_ec2 group
            if cat /tmp/inventory_list.json | jq -e '.aws_ec2' > /dev/null 2>&1; then
              echo "‚úÖ Found aws_ec2 group in inventory"
            else
              echo "‚ö†Ô∏è  Warning: aws_ec2 group not found in inventory (no instances may be running yet)"
            fi
            
            # Check for custom groups (webservers, monitoring)
            if cat /tmp/inventory_list.json | jq -e '.webservers' > /dev/null 2>&1; then
              echo "‚úÖ Found webservers group in inventory"
            else
              echo "‚ö†Ô∏è  Warning: webservers group not found in inventory"
            fi
            
            if cat /tmp/inventory_list.json | jq -e '.monitoring' > /dev/null 2>&1; then
              echo "‚úÖ Found monitoring group in inventory"
            else
              echo "‚ö†Ô∏è  Warning: monitoring group not found in inventory"
            fi
          else
            echo "‚ùå ansible-inventory --list failed"
            exit 1
          fi
          
          echo ""
          
          # Test 2: ansible-inventory --graph
          echo "2Ô∏è‚É£ Testing ansible-inventory --graph..."
          if ansible-inventory -i inventory/aws_ec2.yml --graph; then
            echo "‚úÖ ansible-inventory --graph executed successfully"
          else
            echo "‚ùå ansible-inventory --graph failed"
            exit 1
          fi
          
          echo ""
          echo "‚úÖ AWS EC2 dynamic inventory plugin is working correctly"
          echo "::endgroup::"
      
      - name: üìù Phase 2 - Configure AWS Dynamic Inventory
        run: |
          echo "::group::AWS Dynamic Inventory Configuration"
          cd Ansible
          
          # Verify aws_ec2.yml exists
          if [ ! -f "inventory/aws_ec2.yml" ]; then
            echo "‚ùå inventory/aws_ec2.yml not found"
            exit 1
          fi
          
          echo "‚úÖ AWS EC2 dynamic inventory configured"
          cat inventory/aws_ec2.yml
          echo "::endgroup::"
      
      - name: ‚è≥ Phase 2 - Wait for EC2 Instances Ready
        run: |
          echo "::group::Waiting for EC2 Instances"
          
          # Get instance IDs from Terraform
          cd Terraform
          MONITORING_ID=$(terraform output -raw monitoring_node_id)
          WEBSERVER_IDS=$(terraform output -json webserver_ids | jq -r '.[]')
          
          ALL_IDS="$MONITORING_ID $WEBSERVER_IDS"
          
          echo "Waiting for instances to pass status checks..."
          for instance_id in $ALL_IDS; do
            echo "  Checking $instance_id..."
            aws ec2 wait instance-status-ok --instance-ids $instance_id --region $AWS_REGION
            echo "  ‚úÖ $instance_id is ready"
          done
          
          # Additional wait for SSH to be fully ready
          echo "Waiting additional 30s for SSH daemon to be fully ready..."
          sleep 30
          
          echo "‚úÖ All instances are ready"
          echo "::endgroup::"
      
      - name: ‚úÖ Test - Verify Inventory Structure
        run: |
          echo "::group::Inventory Structure Validation"
          cd Ansible
          
          # Run validation script
          bash ../scripts/validate-inventory.sh
          
          # Display inventory
          echo "üìã Full Inventory:"
          ansible-inventory -i inventory/aws_ec2.yml --list
          
          echo "::endgroup::"
      
      - name: ‚úÖ Test - SSH Connectivity Test
        run: |
          echo "::group::SSH Connectivity Test"
          cd Ansible
          
          # List all hosts
          echo "1Ô∏è‚É£ Listing all hosts in inventory..."
          ansible all -i inventory/aws_ec2.yml --list-hosts
          
          echo ""
          
          # Test SSH connectivity with ping
          echo "2Ô∏è‚É£ Testing SSH connectivity to all hosts..."
          ansible all -i inventory/aws_ec2.yml -m ping -u ubuntu
          
          echo ""
          echo "‚úÖ All hosts are SSH accessible"
          echo "::endgroup::"
      
      # ============================================
      # PHASE 2.5: Docker Pre-Installation
      # ============================================
      - name: üê≥ Phase 2.5 - Install Docker on All Targets
        run: |
          echo "::group::Docker Installation"
          cd Ansible
          
          echo "Installing Docker and Docker Compose on all EC2 instances..."
          ansible-playbook -i inventory/aws_ec2.yml \
            playbooks/install-docker.yml \
            -u ubuntu \
            -v
          
          echo "‚úÖ Docker installed on all targets"
          echo "::endgroup::"
      
      - name: ‚úÖ Test - Verify Docker Installation on All Hosts
        run: |
          echo "::group::Docker Installation Verification"
          cd Ansible
          
          echo "Checking Docker version on all hosts..."
          ansible all -i inventory/aws_ec2.yml \
            -m shell -a 'docker --version' -u ubuntu
          
          echo "Checking Docker Compose version on all hosts..."
          ansible all -i inventory/aws_ec2.yml \
            -m shell -a 'docker compose version' -u ubuntu
          
          echo "Checking Docker service status on all hosts..."
          ansible all -i inventory/aws_ec2.yml \
            -m shell -a 'systemctl is-active docker' -u ubuntu
          
          echo "‚úÖ Docker is installed and running on all hosts"
          echo "::endgroup::"
      
      # ============================================
      # PHASE 3: Monitoring Node Configuration
      # ============================================
      - name: üê≥ Phase 3 - Configure Monitoring Node
        run: |
          echo "::group::Monitoring Node Configuration"
          cd Ansible
          
          echo "Running monitoring node setup playbook..."
          ansible-playbook -i inventory/aws_ec2.yml \
            playbooks/setup-monitoring-node.yml \
            -u ubuntu \
            -v
          
          echo "‚úÖ Monitoring node configured"
          echo "::endgroup::"
      
      - name: ‚úÖ Test - Verify Docker Installation
        run: |
          echo "::group::Docker Installation Verification"
          cd Ansible
          
          echo "Checking Docker version..."
          ansible tag_Name_monitoring_node -i inventory/aws_ec2.yml \
            -m shell -a 'docker --version' -u ubuntu
          
          echo "Checking Docker Compose version..."
          ansible tag_Name_monitoring_node -i inventory/aws_ec2.yml \
            -m shell -a 'docker compose version' -u ubuntu
          
          echo "Checking Docker service status..."
          ansible tag_Name_monitoring_node -i inventory/aws_ec2.yml \
            -m shell -a 'systemctl is-active docker' -u ubuntu
          
          echo "‚úÖ Docker is installed and running"
          echo "::endgroup::"
      
      - name: ‚úÖ Test - Verify Monitoring Containers Running
        run: |
          echo "::group::Monitoring Containers Verification"
          cd Ansible
          
          echo "Checking Docker containers..."
          ansible tag_Name_monitoring_node -i inventory/aws_ec2.yml \
            -m shell -a 'docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"' -u ubuntu
          
          echo "Verifying required containers..."
          CONTAINER_CHECK=$(ansible tag_Name_monitoring_node -i inventory/aws_ec2.yml \
            -m shell -a 'docker ps --format "{{.Names}}"' -u ubuntu -o | grep -E "prometheus|grafana|alertmanager" | wc -l)
          
          if [ "$CONTAINER_CHECK" -ge 3 ]; then
            echo "‚úÖ All monitoring containers are running"
          else
            echo "‚ö†Ô∏è  Warning: Not all containers are running yet"
            echo "   Containers found: $CONTAINER_CHECK/3"
          fi
          
          echo "::endgroup::"
      
      - name: ‚è≥ Phase 3 - Wait for Services Startup
        run: |
          echo "::group::Waiting for Services"
          echo "Waiting 30 seconds for services to fully initialize..."
          sleep 30
          echo "‚úÖ Services should be ready"
          echo "::endgroup::"
      
      - name: ‚úÖ Test - Prometheus Health Check
        run: |
          echo "::group::Prometheus Health Check"
          
          MONITORING_IP="${{ steps.tf_outputs.outputs.monitoring_ip }}"
          
          echo "Testing Prometheus health endpoint..."
          if curl -sf "http://$MONITORING_IP:9090/-/healthy"; then
            echo "‚úÖ Prometheus is healthy"
          else
            echo "‚ùå Prometheus health check failed"
            exit 1
          fi
          
          echo "Testing Prometheus ready endpoint..."
          if curl -sf "http://$MONITORING_IP:9090/-/ready"; then
            echo "‚úÖ Prometheus is ready"
          else
            echo "‚ùå Prometheus ready check failed"
            exit 1
          fi
          
          echo "::endgroup::"
      
      - name: ‚úÖ Test - Grafana Health Check
        run: |
          echo "::group::Grafana Health Check"
          
          MONITORING_IP="${{ steps.tf_outputs.outputs.monitoring_ip }}"
          
          echo "Testing Grafana health endpoint..."
          HEALTH_RESPONSE=$(curl -sf "http://$MONITORING_IP:3000/api/health")
          
          if echo "$HEALTH_RESPONSE" | jq -e '.database == "ok"' > /dev/null; then
            echo "‚úÖ Grafana is healthy"
            echo "   Response: $HEALTH_RESPONSE"
          else
            echo "‚ùå Grafana health check failed"
            echo "   Response: $HEALTH_RESPONSE"
            exit 1
          fi
          
          echo "::endgroup::"
      
      - name: ‚úÖ Test - Alertmanager Health Check
        run: |
          echo "::group::Alertmanager Health Check"
          
          MONITORING_IP="${{ steps.tf_outputs.outputs.monitoring_ip }}"
          
          echo "Testing Alertmanager health endpoint..."
          if curl -sf "http://$MONITORING_IP:9093/-/healthy"; then
            echo "‚úÖ Alertmanager is healthy"
          else
            echo "‚ùå Alertmanager health check failed"
            exit 1
          fi
          
          echo "::endgroup::"
      
      # ============================================
      # PHASE 4: Webserver Configuration
      # ============================================
      - name: üåê Phase 4 - Configure Webservers
        run: |
          echo "::group::Webserver Configuration"
          cd Ansible
          
          echo "Running webserver deployment playbook..."
          ansible-playbook -i inventory/aws_ec2.yml \
            playbooks/deploy-webservers.yml \
            -u ubuntu \
            -v
          
          echo "‚úÖ Webservers configured"
          echo "::endgroup::"
      
      - name: üê≥ Phase 4 - Install Exporters on Webservers
        run: |
          echo "::group::Exporters Installation"
          cd Ansible
          
          echo "Running exporters installation playbook..."
          ansible-playbook -i inventory/aws_ec2.yml \
            playbooks/node-exporter-cadvisor-installation.yml \
            -u ubuntu \
            -v
          
          echo "‚úÖ Exporters installed"
          echo "::endgroup::"
      
      - name: ‚úÖ Test - Verify Apache Installation
        run: |
          echo "::group::Apache Verification"
          cd Ansible
          
          echo "Checking Apache2 service status..."
          ansible tag_Name_webserver* -i inventory/aws_ec2.yml \
            -m service -a 'name=apache2 state=started' -u ubuntu
          
          echo "Verifying Apache2 is enabled..."
          ansible tag_Name_webserver* -i inventory/aws_ec2.yml \
            -m shell -a 'systemctl is-enabled apache2' -u ubuntu
          
          echo "‚úÖ Apache2 is installed and running on all webservers"
          echo "::endgroup::"
      
      - name: ‚úÖ Test - Verify Exporters Running
        run: |
          echo "::group::Exporters Verification"
          cd Ansible
          
          echo "Checking exporter containers on webservers..."
          ansible tag_Name_webserver* -i inventory/aws_ec2.yml \
            -m shell -a 'docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"' -u ubuntu
          
          echo "Verifying node-exporter is running..."
          ansible tag_Name_webserver* -i inventory/aws_ec2.yml \
            -m shell -a 'docker ps | grep -q node-exporter && echo "RUNNING" || echo "NOT RUNNING"' -u ubuntu
          
          echo "Verifying cAdvisor is running..."
          ansible tag_Name_webserver* -i inventory/aws_ec2.yml \
            -m shell -a 'docker ps | grep -q cadvisor && echo "RUNNING" || echo "NOT RUNNING"' -u ubuntu
          
          echo "‚úÖ Exporters are running on all webservers"
          echo "::endgroup::"
      
      - name: ‚úÖ Test - Webserver HTTP Endpoints
        run: |
          echo "::group::Webserver HTTP Endpoint Tests"
          
          WEBSERVER_IPS="${{ steps.tf_outputs.outputs.webserver_ips }}"
          
          echo "Testing webserver endpoints..."
          for ip in $WEBSERVER_IPS; do
            echo "Testing http://$ip/"
            
            RESPONSE=$(curl -sf "http://$ip/" || echo "FAILED")
            
            if [ "$RESPONSE" = "FAILED" ]; then
              echo "‚ùå Webserver $ip is not responding"
              exit 1
            fi
            
            # Check if response contains expected content
            if echo "$RESPONSE" | grep -qi "welcome\|devops\|webserver"; then
              echo "‚úÖ Webserver $ip is working correctly"
            else
              echo "‚ö†Ô∏è  Warning: Webserver $ip responded but content may be unexpected"
            fi
          done
          
          echo "‚úÖ All webservers are accessible"
          echo "::endgroup::"
      
      - name: ‚úÖ Test - Exporter Endpoints
        run: |
          echo "::group::Exporter Endpoint Tests"
          
          WEBSERVER_IPS="${{ steps.tf_outputs.outputs.webserver_ips }}"
          
          echo "Testing exporter endpoints..."
          for ip in $WEBSERVER_IPS; do
            echo "Testing node-exporter on $ip:9100..."
            if curl -sf "http://$ip:9100/metrics" | head -5; then
              echo "‚úÖ Node-exporter on $ip is working"
            else
              echo "‚ùå Node-exporter on $ip failed"
              exit 1
            fi
            
            echo "Testing cAdvisor on $ip:8080..."
            if curl -sf "http://$ip:8080/metrics" | head -5; then
              echo "‚úÖ cAdvisor on $ip is working"
            else
              echo "‚ùå cAdvisor on $ip failed"
              exit 1
            fi
          done
          
          echo "‚úÖ All exporters are accessible"
          echo "::endgroup::"
      
      # ============================================
      # PHASE 5: Monitoring Integration
      # ============================================
      - name: ‚è≥ Phase 5 - Wait for Target Discovery
        run: |
          echo "::group::Waiting for Prometheus Target Discovery"
          echo "Waiting 60 seconds for Prometheus to discover EC2 targets..."
          sleep 60
          echo "‚úÖ Target discovery period complete"
          echo "::endgroup::"
      
      - name: ‚úÖ Test - Prometheus Targets Health
        run: |
          echo "::group::Prometheus Targets Health Check"
          
          MONITORING_IP="${{ steps.tf_outputs.outputs.monitoring_ip }}"
          
          bash scripts/test-prometheus-targets.sh "http://$MONITORING_IP:9090"
          
          echo "::endgroup::"
      
      - name: üìä Phase 5 - Display Prometheus Targets
        run: |
          echo "::group::Prometheus Targets Details"
          
          MONITORING_IP="${{ steps.tf_outputs.outputs.monitoring_ip }}"
          
          echo "Fetching all active targets..."
          curl -s "http://$MONITORING_IP:9090/api/v1/targets" | jq '.data.activeTargets[] | {job: .labels.job, instance: .labels.instance, health: .health, lastScrape: .lastScrape}'
          
          echo "::endgroup::"
      
      - name: üìä Phase 5 - Provision Grafana Dashboards
        run: |
          echo "::group::Grafana Dashboard Provisioning"
          
          MONITORING_IP="${{ steps.tf_outputs.outputs.monitoring_ip }}"
          
          # Check if dashboards directory exists and has files
          if [ -d "Monitoring/grafana-dashboards" ] && [ "$(ls -A Monitoring/grafana-dashboards/*.json 2>/dev/null)" ]; then
            echo "Dashboard files found, provisioning via API..."
            
            # Import dashboards via Grafana API
            for dashboard_file in Monitoring/grafana-dashboards/*.json; do
              echo "Importing $(basename $dashboard_file)..."
              
              # Wrap dashboard JSON in required format
              DASHBOARD_JSON=$(cat $dashboard_file | jq '{dashboard: ., overwrite: true, folderId: 0}')
              
              curl -X POST "http://admin:admin@$MONITORING_IP:3000/api/dashboards/db" \
                -H "Content-Type: application/json" \
                -d "$DASHBOARD_JSON" || echo "Failed to import $(basename $dashboard_file)"
            done
          else
            echo "‚ÑπÔ∏è  No dashboard files found in Monitoring/grafana-dashboards/"
            echo "   Dashboards can be added manually or placed in this directory"
          fi
          
          echo "::endgroup::"
      
      - name: ‚úÖ Test - Verify Grafana Dashboards
        run: |
          echo "::group::Grafana Dashboard Verification"
          
          MONITORING_IP="${{ steps.tf_outputs.outputs.monitoring_ip }}"
          
          bash scripts/test-grafana.sh "http://$MONITORING_IP:3000" admin admin
          
          echo "::endgroup::"
      
      # ============================================
      # PHASE 6: Final Validation & Summary
      # ============================================
      - name: ‚úÖ Phase 6 - Final Smoke Tests
        run: |
          echo "::group::Final Smoke Tests"
          
          MONITORING_IP="${{ steps.tf_outputs.outputs.monitoring_ip }}"
          WEBSERVER_IPS="${{ steps.tf_outputs.outputs.webserver_ips }}"
          
          echo "Running comprehensive final checks..."
          
          # 1. Infrastructure check
          echo "1Ô∏è‚É£ Infrastructure Status:"
          cd Terraform
          terraform show -json | jq -r '.values.root_module.resources[] | select(.type == "aws_instance") | "\(.values.tags.Name): \(.values.instance_state)"'
          cd ..
          
          # 2. Monitoring stack check
          echo ""
          echo "2Ô∏è‚É£ Monitoring Stack:"
          curl -sf "http://$MONITORING_IP:9090/-/healthy" && echo "  ‚úÖ Prometheus: Healthy" || echo "  ‚ùå Prometheus: Failed"
          curl -sf "http://$MONITORING_IP:3000/api/health" && echo "  ‚úÖ Grafana: Healthy" || echo "  ‚ùå Grafana: Failed"
          curl -sf "http://$MONITORING_IP:9093/-/healthy" && echo "  ‚úÖ Alertmanager: Healthy" || echo "  ‚ùå Alertmanager: Failed"
          
          # 3. Webservers check
          echo ""
          echo "3Ô∏è‚É£ Webservers:"
          for ip in $WEBSERVER_IPS; do
            curl -sf "http://$ip/" > /dev/null && echo "  ‚úÖ $ip: Responding" || echo "  ‚ùå $ip: Failed"
          done
          
          # 4. Targets check
          echo ""
          echo "4Ô∏è‚É£ Prometheus Targets:"
          TARGETS_UP=$(curl -s "http://$MONITORING_IP:9090/api/v1/targets" | jq '[.data.activeTargets[] | select(.health == "up")] | length')
          echo "  ‚úÖ Targets UP: $TARGETS_UP"
          
          echo ""
          echo "üéâ All smoke tests passed!"
          echo "::endgroup::"
      
      - name: üìä Phase 6 - Generate Deployment Summary
        run: |
          echo "::group::Deployment Summary"
          
          MONITORING_IP="${{ steps.tf_outputs.outputs.monitoring_ip }}"
          WEBSERVER_IPS="${{ steps.tf_outputs.outputs.webserver_ips }}"
          
          cat << EOF
          ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
          ‚ïë                  üéâ DEPLOYMENT SUCCESSFUL üéâ                  ‚ïë
          ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
          
          üìç INFRASTRUCTURE ENDPOINTS
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          üîç Prometheus:    http://$MONITORING_IP:9090
          üìä Grafana:       http://$MONITORING_IP:3000
             ‚îî‚îÄ Username:   admin
             ‚îî‚îÄ Password:   admin
          üö® Alertmanager:  http://$MONITORING_IP:9093
          
          üåê WEBSERVERS
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          EOF
          
          idx=0
          for ip in $WEBSERVER_IPS; do
            echo "  Webserver-$idx:  http://$ip"
            idx=$((idx + 1))
          done
          
          cat << EOF
          
          üìà MONITORING STATUS
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          EOF
          
          TARGETS_UP=$(curl -s "http://$MONITORING_IP:9090/api/v1/targets" | jq '[.data.activeTargets[] | select(.health == "up")] | length')
          TARGETS_TOTAL=$(curl -s "http://$MONITORING_IP:9090/api/v1/targets" | jq '.data.activeTargets | length')
          
          echo "  Targets UP:      $TARGETS_UP / $TARGETS_TOTAL"
          
          DASHBOARDS_COUNT=$(curl -s -u admin:admin "http://$MONITORING_IP:3000/api/search?type=dash-db" | jq 'length')
          echo "  Dashboards:      $DASHBOARDS_COUNT"
          
          cat << EOF
          
          ‚ö° QUICK ACTIONS
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          View Targets:     http://$MONITORING_IP:9090/targets
          View Alerts:      http://$MONITORING_IP:9090/alerts
          View Dashboards:  http://$MONITORING_IP:3000/dashboards
          
          ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
          ‚ïë  All services are operational and ready for monitoring! üöÄ    ‚ïë
          ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
          EOF
          
          echo "::endgroup::"
      
      - name: üéâ Phase 6 - Deployment Complete
        run: |
          echo "‚úÖ Deployment workflow completed successfully!"
          echo "   All phases passed validation."
          echo "   Infrastructure is ready for use."
  
  # ============================================
  # FAILURE NOTIFICATION: Runs only on failure
  # ============================================
  notify_failure:
    runs-on: ubuntu-latest
    needs: deploy
    if: failure()
    name: Deployment Failure - Manual Action Required
    
    steps:
      - name: üìã Checkout Code
        uses: actions/checkout@v4
      
      - name: üîê Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: üèóÔ∏è Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false
      
      - name: üìä Assess Current State
        continue-on-error: true
        run: |
          echo "::group::Current Infrastructure State"
          cd Terraform
          
          echo "Attempting to check current state..."
          
          # Try to initialize
          if terraform init \
            -backend-config="bucket=${{ secrets.TF_BACKEND_BUCKET }}" \
            -backend-config="dynamodb_table=${{ secrets.TF_BACKEND_DYNAMODB_TABLE }}" \
            -backend-config="region=${{ secrets.TF_BACKEND_REGION }}"; then
            
            echo ""
            echo "üìã Resources currently in Terraform state:"
            terraform state list || echo "No resources in state or state is corrupted"
            
            echo ""
            echo "üìã Resource count:"
            RESOURCE_COUNT=$(terraform state list 2>/dev/null | wc -l || echo "0")
            echo "   $RESOURCE_COUNT resources tracked"
          else
            echo "‚ö†Ô∏è  Cannot initialize Terraform to check state"
            echo "   This may indicate the backend doesn't exist or is misconfigured"
          fi
          
          echo ""
          echo "üìã EC2 Instances in AWS (tagged ManagedBy=Terraform):"
          aws ec2 describe-instances \
            --region ${{ env.AWS_REGION }} \
            --filters "Name=tag:ManagedBy,Values=Terraform" \
                      "Name=instance-state-name,Values=running,stopped,stopping,pending" \
            --query 'Reservations[*].Instances[*].[InstanceId,Tags[?Key==`Name`].Value|[0],State.Name,PublicIpAddress]' \
            --output table || echo "No instances found or connection issue"
          
          echo "::endgroup::"
      
      - name: ‚ö†Ô∏è Failure Instructions
        run: |
          cat <<'EOF'
          ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
          ‚ïë        ‚ö†Ô∏è  DEPLOYMENT FAILED - MANUAL ACTION REQUIRED     ‚ïë
          ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
          
          üö® The deployment has failed. Partial infrastructure may exist in AWS.
          
          ‚ö†Ô∏è  IMPORTANT: Automatic cleanup has been DISABLED to prevent:
             ‚Ä¢ State file corruption
             ‚Ä¢ Orphaned "zombie" resources
             ‚Ä¢ Cascading failures
          
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          üìù RECOMMENDED ACTIONS:
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          
          1Ô∏è‚É£ REVIEW THE FAILURE
             Review the logs above to identify what caused the failure:
             ‚Ä¢ Quota limits exceeded?
             ‚Ä¢ Permission issues?
             ‚Ä¢ Invalid configuration?
             ‚Ä¢ Network/timeout issues?
          
          2Ô∏è‚É£ CHECK CURRENT STATE
             Check what resources currently exist:
             
             In your local terminal:
             $ cd Terraform
             $ terraform init \
                 -backend-config="bucket=${{ secrets.TF_BACKEND_BUCKET }}" \
                 -backend-config="dynamodb_table=${{ secrets.TF_BACKEND_DYNAMODB_TABLE }}" \
                 -backend-config="region=${{ secrets.TF_BACKEND_REGION }}"
             $ terraform state list
             
             Or check AWS Console manually.
          
          3Ô∏è‚É£ CHOOSE A RECOVERY PATH
          
             üìå OPTION A - FIX AND RETRY (Recommended)
             ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
             If the failure was due to a fixable issue (typo, permission, etc.):
             
             1. Fix the underlying issue in your code or configuration
             2. Commit and push the fix
             3. Re-run this "Deploy Infrastructure & Services" workflow
             4. Terraform will pick up where it left off using the state file
             
             This is the SAFEST option.
          
             üìå OPTION B - CLEAN DESTROY
             ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
             If you want to completely tear down and start fresh:
             
             1. Run the "Destroy All Infrastructure" workflow
             2. It includes proper state refresh and retry logic
             3. Wait for successful completion
             4. Fix the issue that caused the failure
             5. Re-run deployment
             
             Note: The destroy workflow preserves the S3 state bucket.
          
             üìå OPTION C - MANUAL CLEANUP
             ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
             For advanced users who need fine-grained control:
             
             1. Identify orphaned resources:
                $ bash scripts/list-orphaned-resources.sh
             
             2. For resources in AWS but NOT in state:
                $ terraform import <resource_type>.<name> <aws_id>
                Example: terraform import aws_instance.web i-1234567890abcdef0
             
             3. For resources in state but NOT in AWS:
                $ terraform state rm <resource_type>.<name>
                Example: terraform state rm aws_instance.web
             
             4. Review our state management guide:
                See: scripts/state-management-guide.md
          
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          üîó HELPFUL RESOURCES:
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          
          ‚Ä¢ State Management Guide: scripts/state-management-guide.md
          ‚Ä¢ List Orphaned Resources: scripts/list-orphaned-resources.sh
          ‚Ä¢ Terraform State Commands: https://developer.hashicorp.com/terraform/cli/commands/state
          
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          ‚ö†Ô∏è  WHY NO AUTO-CLEANUP?
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          
          Running terraform destroy immediately after a failure can:
          ‚Ä¢ Corrupt the state file if destroy itself fails
          ‚Ä¢ Leave "zombie" resources (in AWS but not in state)
          ‚Ä¢ Make recovery harder by destroying diagnostic information
          ‚Ä¢ Prevent proper root cause analysis
          
          Manual intervention ensures you understand what happened and
          can recover properly without making the situation worse.
          
          ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
          ‚ïë  Need Help? Check the workflow logs and state management  ‚ïë
          ‚ïë  guide above, or review the Terraform documentation.      ‚ïë
          ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
          EOF
