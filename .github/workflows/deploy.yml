name: Deploy Infrastructure & Services

on:
  workflow_dispatch:  # Manual trigger
  # Uncomment after testing to enable on push
  # push:
  #   branches:
  #     - main

env:
  AWS_REGION: us-east-1
  TERRAFORM_VERSION: 1.6.0
  ANSIBLE_VERSION: 2.15.0

jobs:
  deploy:
    runs-on: ubuntu-latest
    name: Deploy Infrastructure & Services
    
    steps:
      # ============================================
      # PHASE 0: Setup & Pre-flight Checks
      # ============================================
      - name: ğŸ“‹ Phase 0 - Checkout Code
        uses: actions/checkout@v4
      
      - name: âœ… Test - Verify Repository Structure
        run: |
          echo "::group::Repository Structure Check"
          echo "Checking required directories..."
          
          required_dirs=("Terraform" "Ansible" "Monitoring" "scripts")
          for dir in "${required_dirs[@]}"; do
            if [ -d "$dir" ]; then
              echo "âœ… Found: $dir"
            else
              echo "âŒ Missing: $dir"
              exit 1
            fi
          done
          
          echo "âœ… Repository structure validated"
          echo "::endgroup::"
      
      - name: ğŸ” Phase 0 - Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: âœ… Test - Verify AWS Credentials
        run: |
          echo "::group::AWS Credentials Verification"
          echo "Testing AWS credentials..."
          
          CALLER_IDENTITY=$(aws sts get-caller-identity)
          ACCOUNT_ID=$(echo $CALLER_IDENTITY | jq -r '.Account')
          USER_ARN=$(echo $CALLER_IDENTITY | jq -r '.Arn')
          
          echo "âœ… AWS Account: $ACCOUNT_ID"
          echo "âœ… IAM Identity: $USER_ARN"
          echo "âœ… AWS Region: $AWS_REGION"
          echo "âœ… AWS credentials are valid"
          echo "::endgroup::"
      
      - name: ğŸ”‘ Phase 0 - Setup SSH Agent
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}
      
      - name: âœ… Test - Verify SSH Key Loaded
        run: |
          echo "::group::SSH Key Verification"
          echo "Checking SSH agent..."
          
          KEY_COUNT=$(ssh-add -L | wc -l)
          if [ "$KEY_COUNT" -ge 1 ]; then
            echo "âœ… SSH key loaded successfully"
            echo "   Key fingerprint:"
            ssh-add -l | head -1
          else
            echo "âŒ No SSH keys found in agent"
            exit 1
          fi
          
          # Configure SSH to not check host keys (for automation)
          mkdir -p ~/.ssh
          cat >> ~/.ssh/config <<EOF
          Host *
            StrictHostKeyChecking no
            UserKnownHostsFile=/dev/null
            LogLevel ERROR
          EOF
          
          echo "âœ… SSH configured for automation"
          echo "::endgroup::"
      
      # ============================================
      # PHASE 1: Infrastructure Provisioning
      # ============================================
      - name: ğŸ—ï¸ Phase 1 - Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false  # Required for output parsing
      
      - name: ğŸ—„ï¸ Phase 1 - Ensure Backend Infrastructure Exists
        run: |
          echo "::group::Backend Infrastructure Setup"
          
          # Get AWS Account ID
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          BUCKET_NAME="devops-project-terraform-state-$ACCOUNT_ID"
          TABLE_NAME="terraform-state-locks"
          
          echo "ğŸ“‹ Target backend resources:"
          echo "   S3 Bucket: $BUCKET_NAME"
          echo "   DynamoDB Table: $TABLE_NAME"
          echo ""
          
          # ============================================
          # Create S3 Bucket if it doesn't exist
          # ============================================
          echo "Checking S3 bucket..."
          if aws s3 ls "s3://$BUCKET_NAME" 2>/dev/null; then
            echo "âœ… S3 bucket already exists: $BUCKET_NAME"
          else
            echo "ğŸ“¦ Creating S3 bucket: $BUCKET_NAME"
            aws s3api create-bucket \
              --bucket "$BUCKET_NAME" \
              --region us-east-1
            
            # Enable versioning
            echo "   Enabling versioning..."
            aws s3api put-bucket-versioning \
              --bucket "$BUCKET_NAME" \
              --versioning-configuration Status=Enabled
            
            # Enable encryption
            echo "   Enabling encryption..."
            aws s3api put-bucket-encryption \
              --bucket "$BUCKET_NAME" \
              --server-side-encryption-configuration '{
                "Rules": [{
                  "ApplyServerSideEncryptionByDefault": {
                    "SSEAlgorithm": "AES256"
                  }
                }]
              }'
            
            # Block public access
            echo "   Blocking public access..."
            aws s3api put-public-access-block \
              --bucket "$BUCKET_NAME" \
              --public-access-block-configuration \
                "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true"
            
            # Add tags
            echo "   Adding tags..."
            aws s3api put-bucket-tagging \
              --bucket "$BUCKET_NAME" \
              --tagging 'TagSet=[
                {Key=Name,Value="Terraform State Bucket"},
                {Key=Environment,Value="Production"},
                {Key=ManagedBy,Value="GitHub Actions"}
              ]'
            
            echo "âœ… S3 bucket created successfully"
          fi
          
          # ============================================
          # Create DynamoDB Table if it doesn't exist
          # ============================================
          echo ""
          echo "Checking DynamoDB table..."
          if aws dynamodb describe-table --table-name "$TABLE_NAME" 2>/dev/null; then
            echo "âœ… DynamoDB table already exists: $TABLE_NAME"
          else
            echo "ğŸ“Š Creating DynamoDB table: $TABLE_NAME"
            aws dynamodb create-table \
              --table-name "$TABLE_NAME" \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST \
              --tags \
                Key=Name,Value="Terraform State Lock Table" \
                Key=Environment,Value="Production" \
                Key=ManagedBy,Value="GitHub Actions"
            
            echo "   Waiting for table to be active..."
            aws dynamodb wait table-exists --table-name "$TABLE_NAME"
            
            echo "âœ… DynamoDB table created successfully"
          fi
          
          echo ""
          echo "âœ… Backend infrastructure is ready"
          echo "::endgroup::"
      
      - name: ğŸ”“ Phase 1 - Clear Stuck Locks (if any)
        if: always()
        continue-on-error: true
        run: |
          echo "::group::Lock Cleanup"
          
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          BUCKET_NAME="devops-project-terraform-state-$ACCOUNT_ID"
          
          echo "Checking for stuck locks..."
          aws dynamodb delete-item \
            --table-name terraform-state-locks \
            --key "{\"LockID\": {\"S\": \"$BUCKET_NAME/devops-project/terraform.tfstate\"}}" \
            2>/dev/null || echo "No stuck locks found (or table doesn't exist yet)"
          
          echo "::endgroup::"
      
      - name: ğŸ—ï¸ Phase 1 - Terraform Init
        run: |
          echo "::group::Terraform Initialization"
          cd Terraform
          
          echo "Initializing Terraform with remote backend..."
          echo "Backend: s3://${{ secrets.TF_BACKEND_BUCKET }}"
          echo "Locking: DynamoDB table ${{ secrets.TF_BACKEND_DYNAMODB_TABLE }}"
          echo ""
          
          terraform init \
            -backend-config="bucket=${{ secrets.TF_BACKEND_BUCKET }}" \
            -backend-config="dynamodb_table=${{ secrets.TF_BACKEND_DYNAMODB_TABLE }}" \
            -backend-config="region=${{ secrets.TF_BACKEND_REGION }}" \
            -input=false
          
          echo "âœ… Terraform initialized successfully"
          echo "::endgroup::"
      
      - name: ğŸ—ï¸ Phase 1 - Terraform Validate
        run: |
          echo "::group::Terraform Validation"
          cd Terraform
          terraform validate
          echo "::endgroup::"
      
      - name: âœ… Test - Terraform Validation Success
        run: |
          echo "::group::Terraform Validation Test"
          cd Terraform
          
          # Check format
          echo "Checking Terraform formatting..."
          terraform fmt -check -recursive || {
            echo "âš ï¸  Warning: Terraform files are not formatted"
            echo "   Run 'terraform fmt -recursive' to fix"
          }
          
          # Validate again to confirm
          if terraform validate > /dev/null 2>&1; then
            echo "âœ… Terraform configuration is valid"
          else
            echo "âŒ Terraform validation failed"
            exit 1
          fi
          echo "::endgroup::"
      
      - name: ğŸ”„ Phase 1 - Refresh State (Detect Drift)
        run: |
          echo "::group::State Refresh"
          cd Terraform
          
          echo "Refreshing Terraform state to detect any manual changes or drift..."
          echo "This will update the state to match the real-world infrastructure."
          echo ""
          
          terraform apply -refresh-only \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -auto-approve || {
            echo "âš ï¸  State refresh encountered issues"
            echo "   This may indicate:"
            echo "   - Resources were manually deleted from AWS"
            echo "   - References to non-existent resources in state"
            echo "   - Permission issues"
            echo ""
            echo "   Terraform will attempt to reconcile during plan/apply"
          }
          
          echo ""
          echo "âœ… State refresh complete"
          echo "::endgroup::"
      
      - name: ğŸ—ï¸ Phase 1 - Terraform Plan
        run: |
          echo "::group::Terraform Plan"
          cd Terraform
          terraform plan -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" -out=tfplan
          echo "::endgroup::"
      
      - name: ğŸ—ï¸ Phase 1 - Terraform Apply
        run: |
          echo "::group::Terraform Apply"
          cd Terraform
          terraform apply -auto-approve tfplan
          echo "::endgroup::"
      
      - name: âœ… Test - Terraform Outputs Sanity Check
        id: tf_outputs
        run: |
          echo "::group::Terraform Outputs Validation"
          cd Terraform
          
          # Run validation script
          bash ../scripts/validate-terraform-outputs.sh
          
          # Save outputs for later steps
          MONITORING_IP=$(terraform output -raw monitoring_node_public_ip)
          WEBSERVER_IPS=$(terraform output -json webserver_public_ips | jq -r '.[]' | tr '\n' ' ')
          
          echo "monitoring_ip=$MONITORING_IP" >> $GITHUB_OUTPUT
          echo "webserver_ips=$WEBSERVER_IPS" >> $GITHUB_OUTPUT
          
          echo "::endgroup::"
      
      - name: ğŸ“Š Phase 1 - Save Terraform Outputs
        run: |
          echo "::group::Saving Terraform Outputs"
          cd Terraform
          
          terraform output -json > terraform-outputs.json
          
          echo "ğŸ“„ Terraform Outputs:"
          cat terraform-outputs.json | jq '.'
          
          echo "::endgroup::"
      
      - name: ğŸ“¤ Upload Terraform Outputs
        uses: actions/upload-artifact@v4
        with:
          name: terraform-outputs
          path: Terraform/terraform-outputs.json
          retention-days: 7
      
      # ============================================
      # PHASE 2: Ansible Environment Setup
      # ============================================
      - name: ğŸš€ Phase 2 - Ansible Environment Setup
        run: |
          echo "========================================="
          echo "Phase 2: Ansible Environment Setup"
          echo "========================================="
      
      - name: ğŸ” Phase 2.1 - Verify Pre-installed Tools
        run: |
          echo "::group::Pre-installed Versions (Before Installation)"
          echo "=== Pre-installed Versions (Before Installation) ==="
          echo ""
          echo "Python: $(python3 --version 2>&1 || echo 'Not found')"
          echo "Pip: $(pip3 --version 2>&1 || echo 'Not found')"
          echo "AWS CLI: $(aws --version 2>&1 || echo 'Not found')"
          echo "Ansible (if pre-installed): $(ansible --version 2>&1 | head -n 1 || echo 'Not found')"
          echo ""
          echo "::endgroup::"
      
      - name: ğŸ“¦ Phase 2.2 - Install Python Dependencies
        run: |
          echo "::group::Installing Python Packages"
          echo "=== Installing Python Packages ==="
          
          # Upgrade pip first
          echo "Upgrading pip..."
          python3 -m pip install --upgrade pip
          
          # Install boto3 and botocore with specific version requirements
          echo ""
          echo "Installing boto3 and botocore..."
          python3 -m pip install 'boto3>=1.34.0' 'botocore>=1.34.0'
          
          # Install Ansible (this installs to system Python, NOT pipx)
          echo ""
          echo "Installing Ansible..."
          python3 -m pip install 'ansible>=2.17'
          
          echo ""
          echo "âœ… Python packages installed successfully"
          echo "::endgroup::"
      
      - name: ğŸ¯ Phase 2.3 - Install Ansible Collections
        run: |
          echo "::group::Installing Ansible Collections"
          echo "=== Installing Ansible Collections ==="
          
          # Install amazon.aws collection
          ansible-galaxy collection install amazon.aws
          
          echo ""
          echo "âœ… Ansible collections installed successfully"
          echo "::endgroup::"
      
      - name: âœ… Phase 2.4 - Verify Complete Installation
        run: |
          echo "::group::Installation Verification"
          echo "========================================="
          echo "Installation Verification"
          echo "========================================="
          
          echo ""
          echo "=== Python Version ==="
          python3 --version
          
          echo ""
          echo "=== Pip Version ==="
          pip3 --version
          
          echo ""
          echo "=== Ansible Version and Python Interpreter ==="
          ansible --version
          
          echo ""
          echo "=== Boto3 Version ==="
          python3 -c "import boto3; print(f'boto3: {boto3.__version__}')"
          
          echo ""
          echo "=== Botocore Version ==="
          python3 -c "import botocore; print(f'botocore: {botocore.__version__}')"
          
          echo ""
          echo "=== AWS CLI Version ==="
          aws --version
          
          echo ""
          echo "=== Installed Ansible Collections ==="
          ansible-galaxy collection list amazon.aws
          
          echo ""
          echo "=== Test AWS EC2 Inventory Plugin Documentation ==="
          ansible-doc -t inventory amazon.aws.aws_ec2 | head -n 5
          
          echo ""
          echo "âœ… All installations verified successfully"
          echo "::endgroup::"
      
      - name: ğŸ”— Phase 2.5 - Test AWS Connectivity
        run: |
          echo "::group::AWS Connectivity Test"
          echo "=== Testing AWS Connectivity ==="
          
          echo "Testing AWS EC2 API access..."
          aws ec2 describe-instances --region ${{ env.AWS_REGION }} --query 'Reservations[*].Instances[*].[InstanceId,State.Name,Tags[?Key==`Name`].Value|[0]]' --output table || echo "No instances found or connection issue"
          
          echo ""
          echo "âœ… AWS connectivity verified"
          echo "::endgroup::"
      
      - name: âœ… Test - Verify AWS EC2 Dynamic Inventory Plugin
        run: |
          echo "::group::AWS EC2 Plugin Verification"
          
          cd Ansible
          
          # Verify aws_ec2.yml exists
          if [ ! -f "inventory/aws_ec2.yml" ]; then
            echo "âŒ inventory/aws_ec2.yml not found"
            exit 1
          fi
          
          echo "Testing AWS EC2 dynamic inventory plugin..."
          echo ""
          
          # Test 1: ansible-inventory --list
          echo "1ï¸âƒ£ Testing ansible-inventory --list..."
          if ansible-inventory -i inventory/aws_ec2.yml --list > /tmp/inventory_list.json; then
            echo "âœ… ansible-inventory --list executed successfully"
            
            # Check if output contains aws_ec2 group
            if cat /tmp/inventory_list.json | jq -e '.aws_ec2' > /dev/null 2>&1; then
              echo "âœ… Found aws_ec2 group in inventory"
            else
              echo "âš ï¸  Warning: aws_ec2 group not found in inventory (no instances may be running yet)"
            fi
          else
            echo "âŒ ansible-inventory --list failed"
            exit 1
          fi
          
          echo ""
          
          # Test 2: ansible-inventory --graph
          echo "2ï¸âƒ£ Testing ansible-inventory --graph..."
          if ansible-inventory -i inventory/aws_ec2.yml --graph; then
            echo "âœ… ansible-inventory --graph executed successfully"
          else
            echo "âŒ ansible-inventory --graph failed"
            exit 1
          fi
          
          echo ""
          echo "âœ… AWS EC2 dynamic inventory plugin is working correctly"
          echo "::endgroup::"
      
      - name: ğŸ“ Phase 2 - Configure AWS Dynamic Inventory
        run: |
          echo "::group::AWS Dynamic Inventory Configuration"
          cd Ansible
          
          # Verify aws_ec2.yml exists
          if [ ! -f "inventory/aws_ec2.yml" ]; then
            echo "âŒ inventory/aws_ec2.yml not found"
            exit 1
          fi
          
          echo "âœ… AWS EC2 dynamic inventory configured"
          cat inventory/aws_ec2.yml
          echo "::endgroup::"
      
      - name: â³ Phase 2 - Wait for EC2 Instances Ready
        run: |
          echo "::group::Waiting for EC2 Instances"
          
          # Get instance IDs from Terraform
          cd Terraform
          MONITORING_ID=$(terraform output -raw monitoring_node_id)
          WEBSERVER_IDS=$(terraform output -json webserver_ids | jq -r '.[]')
          
          ALL_IDS="$MONITORING_ID $WEBSERVER_IDS"
          
          echo "Waiting for instances to pass status checks..."
          for instance_id in $ALL_IDS; do
            echo "  Checking $instance_id..."
            aws ec2 wait instance-status-ok --instance-ids $instance_id --region $AWS_REGION
            echo "  âœ… $instance_id is ready"
          done
          
          # Additional wait for SSH to be fully ready
          echo "Waiting additional 30s for SSH daemon to be fully ready..."
          sleep 30
          
          echo "âœ… All instances are ready"
          echo "::endgroup::"
      
      - name: âœ… Test - Verify Inventory Structure
        run: |
          echo "::group::Inventory Structure Validation"
          cd Ansible
          
          # Run validation script
          bash ../scripts/validate-inventory.sh
          
          # Display inventory
          echo "ğŸ“‹ Full Inventory:"
          ansible-inventory -i inventory/aws_ec2.yml --list
          
          echo "::endgroup::"
      
      - name: âœ… Test - SSH Connectivity Test
        run: |
          echo "::group::SSH Connectivity Test"
          cd Ansible
          
          # List all hosts
          echo "1ï¸âƒ£ Listing all hosts in inventory..."
          ansible all -i inventory/aws_ec2.yml --list-hosts
          
          echo ""
          
          # Test SSH connectivity with ping
          echo "2ï¸âƒ£ Testing SSH connectivity to all hosts..."
          ansible all -i inventory/aws_ec2.yml -m ping -u ubuntu
          
          echo ""
          echo "âœ… All hosts are SSH accessible"
          echo "::endgroup::"
      
      # ============================================
      # PHASE 2.5: Docker Pre-Installation
      # ============================================
      - name: ğŸ³ Phase 2.5 - Install Docker on All Targets
        run: |
          echo "::group::Docker Installation"
          cd Ansible
          
          echo "Installing Docker and Docker Compose on all EC2 instances..."
          ansible-playbook -i inventory/aws_ec2.yml \
            playbooks/install-docker.yml \
            -u ubuntu \
            -v
          
          echo "âœ… Docker installed on all targets"
          echo "::endgroup::"
      
      - name: âœ… Test - Verify Docker Installation on All Hosts
        run: |
          echo "::group::Docker Installation Verification"
          cd Ansible
          
          echo "Checking Docker version on all hosts..."
          ansible all -i inventory/aws_ec2.yml \
            -m shell -a 'docker --version' -u ubuntu
          
          echo "Checking Docker Compose version on all hosts..."
          ansible all -i inventory/aws_ec2.yml \
            -m shell -a 'docker compose version' -u ubuntu
          
          echo "Checking Docker service status on all hosts..."
          ansible all -i inventory/aws_ec2.yml \
            -m shell -a 'systemctl is-active docker' -u ubuntu
          
          echo "âœ… Docker is installed and running on all hosts"
          echo "::endgroup::"
      
      # ============================================
      # PHASE 3: Monitoring Node Configuration
      # ============================================
      - name: ğŸ³ Phase 3 - Configure Monitoring Node
        run: |
          echo "::group::Monitoring Node Configuration"
          cd Ansible
          
          echo "Running monitoring node setup playbook..."
          ansible-playbook -i inventory/aws_ec2.yml \
            playbooks/setup-monitoring-node.yml \
            -u ubuntu \
            -v
          
          echo "âœ… Monitoring node configured"
          echo "::endgroup::"
      
      - name: âœ… Test - Verify Docker Installation
        run: |
          echo "::group::Docker Installation Verification"
          cd Ansible
          
          echo "Checking Docker version..."
          ansible tag_Name_monitoring_node -i inventory/aws_ec2.yml \
            -m shell -a 'docker --version' -u ubuntu
          
          echo "Checking Docker Compose version..."
          ansible tag_Name_monitoring_node -i inventory/aws_ec2.yml \
            -m shell -a 'docker compose version' -u ubuntu
          
          echo "Checking Docker service status..."
          ansible tag_Name_monitoring_node -i inventory/aws_ec2.yml \
            -m shell -a 'systemctl is-active docker' -u ubuntu
          
          echo "âœ… Docker is installed and running"
          echo "::endgroup::"
      
      - name: âœ… Test - Verify Monitoring Containers Running
        run: |
          echo "::group::Monitoring Containers Verification"
          cd Ansible
          
          echo "Checking Docker containers..."
          ansible tag_Name_monitoring_node -i inventory/aws_ec2.yml \
            -m shell -a 'docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"' -u ubuntu
          
          echo "Verifying required containers..."
          CONTAINER_CHECK=$(ansible tag_Name_monitoring_node -i inventory/aws_ec2.yml \
            -m shell -a 'docker ps --format "{{.Names}}"' -u ubuntu -o | grep -E "prometheus|grafana|alertmanager" | wc -l)
          
          if [ "$CONTAINER_CHECK" -ge 3 ]; then
            echo "âœ… All monitoring containers are running"
          else
            echo "âš ï¸  Warning: Not all containers are running yet"
            echo "   Containers found: $CONTAINER_CHECK/3"
          fi
          
          echo "::endgroup::"
      
      - name: â³ Phase 3 - Wait for Services Startup
        run: |
          echo "::group::Waiting for Services"
          echo "Waiting 30 seconds for services to fully initialize..."
          sleep 30
          echo "âœ… Services should be ready"
          echo "::endgroup::"
      
      - name: âœ… Test - Prometheus Health Check
        run: |
          echo "::group::Prometheus Health Check"
          
          MONITORING_IP="${{ steps.tf_outputs.outputs.monitoring_ip }}"
          
          echo "Testing Prometheus health endpoint..."
          if curl -sf "http://$MONITORING_IP:9090/-/healthy"; then
            echo "âœ… Prometheus is healthy"
          else
            echo "âŒ Prometheus health check failed"
            exit 1
          fi
          
          echo "Testing Prometheus ready endpoint..."
          if curl -sf "http://$MONITORING_IP:9090/-/ready"; then
            echo "âœ… Prometheus is ready"
          else
            echo "âŒ Prometheus ready check failed"
            exit 1
          fi
          
          echo "::endgroup::"
      
      - name: âœ… Test - Grafana Health Check
        run: |
          echo "::group::Grafana Health Check"
          
          MONITORING_IP="${{ steps.tf_outputs.outputs.monitoring_ip }}"
          
          echo "Testing Grafana health endpoint..."
          HEALTH_RESPONSE=$(curl -sf "http://$MONITORING_IP:3000/api/health")
          
          if echo "$HEALTH_RESPONSE" | jq -e '.database == "ok"' > /dev/null; then
            echo "âœ… Grafana is healthy"
            echo "   Response: $HEALTH_RESPONSE"
          else
            echo "âŒ Grafana health check failed"
            echo "   Response: $HEALTH_RESPONSE"
            exit 1
          fi
          
          echo "::endgroup::"
      
      - name: âœ… Test - Alertmanager Health Check
        run: |
          echo "::group::Alertmanager Health Check"
          
          MONITORING_IP="${{ steps.tf_outputs.outputs.monitoring_ip }}"
          
          echo "Testing Alertmanager health endpoint..."
          if curl -sf "http://$MONITORING_IP:9093/-/healthy"; then
            echo "âœ… Alertmanager is healthy"
          else
            echo "âŒ Alertmanager health check failed"
            exit 1
          fi
          
          echo "::endgroup::"
      
      # ============================================
      # PHASE 4: Webserver Configuration
      # ============================================
      - name: ğŸŒ Phase 4 - Configure Webservers
        run: |
          echo "::group::Webserver Configuration"
          cd Ansible
          
          echo "Running webserver deployment playbook..."
          ansible-playbook -i inventory/aws_ec2.yml \
            playbooks/deploy-webservers.yml \
            -u ubuntu \
            -v
          
          echo "âœ… Webservers configured"
          echo "::endgroup::"
      
      - name: ğŸ³ Phase 4 - Install Exporters on Webservers
        run: |
          echo "::group::Exporters Installation"
          cd Ansible
          
          echo "Running exporters installation playbook..."
          ansible-playbook -i inventory/aws_ec2.yml \
            playbooks/node-exporter-cadvisor-installation.yml \
            -u ubuntu \
            -v
          
          echo "âœ… Exporters installed"
          echo "::endgroup::"
      
      - name: âœ… Test - Verify Apache Installation
        run: |
          echo "::group::Apache Verification"
          cd Ansible
          
          echo "Checking Apache2 service status..."
          ansible tag_Name_webserver* -i inventory/aws_ec2.yml \
            -m service -a 'name=apache2 state=started' -u ubuntu
          
          echo "Verifying Apache2 is enabled..."
          ansible tag_Name_webserver* -i inventory/aws_ec2.yml \
            -m shell -a 'systemctl is-enabled apache2' -u ubuntu
          
          echo "âœ… Apache2 is installed and running on all webservers"
          echo "::endgroup::"
      
      - name: âœ… Test - Verify Exporters Running
        run: |
          echo "::group::Exporters Verification"
          cd Ansible
          
          echo "Checking exporter containers on webservers..."
          ansible tag_Name_webserver* -i inventory/aws_ec2.yml \
            -m shell -a 'docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"' -u ubuntu
          
          echo "Verifying node-exporter is running..."
          ansible tag_Name_webserver* -i inventory/aws_ec2.yml \
            -m shell -a 'docker ps | grep -q node-exporter && echo "RUNNING" || echo "NOT RUNNING"' -u ubuntu
          
          echo "Verifying cAdvisor is running..."
          ansible tag_Name_webserver* -i inventory/aws_ec2.yml \
            -m shell -a 'docker ps | grep -q cadvisor && echo "RUNNING" || echo "NOT RUNNING"' -u ubuntu
          
          echo "âœ… Exporters are running on all webservers"
          echo "::endgroup::"
      
      - name: âœ… Test - Webserver HTTP Endpoints
        run: |
          echo "::group::Webserver HTTP Endpoint Tests"
          
          WEBSERVER_IPS="${{ steps.tf_outputs.outputs.webserver_ips }}"
          
          echo "Testing webserver endpoints..."
          for ip in $WEBSERVER_IPS; do
            echo "Testing http://$ip/"
            
            RESPONSE=$(curl -sf "http://$ip/" || echo "FAILED")
            
            if [ "$RESPONSE" = "FAILED" ]; then
              echo "âŒ Webserver $ip is not responding"
              exit 1
            fi
            
            # Check if response contains expected content
            if echo "$RESPONSE" | grep -qi "welcome\|devops\|webserver"; then
              echo "âœ… Webserver $ip is working correctly"
            else
              echo "âš ï¸  Warning: Webserver $ip responded but content may be unexpected"
            fi
          done
          
          echo "âœ… All webservers are accessible"
          echo "::endgroup::"
      
      - name: âœ… Test - Exporter Endpoints
        run: |
          echo "::group::Exporter Endpoint Tests"
          
          WEBSERVER_IPS="${{ steps.tf_outputs.outputs.webserver_ips }}"
          
          echo "Testing exporter endpoints..."
          for ip in $WEBSERVER_IPS; do
            echo "Testing node-exporter on $ip:9100..."
            if curl -sf "http://$ip:9100/metrics" | head -5; then
              echo "âœ… Node-exporter on $ip is working"
            else
              echo "âŒ Node-exporter on $ip failed"
              exit 1
            fi
            
            echo "Testing cAdvisor on $ip:8080..."
            if curl -sf "http://$ip:8080/metrics" | head -5; then
              echo "âœ… cAdvisor on $ip is working"
            else
              echo "âŒ cAdvisor on $ip failed"
              exit 1
            fi
          done
          
          echo "âœ… All exporters are accessible"
          echo "::endgroup::"
      
      # ============================================
      # PHASE 5: Monitoring Integration
      # ============================================
      - name: â³ Phase 5 - Wait for Target Discovery
        run: |
          echo "::group::Waiting for Prometheus Target Discovery"
          echo "Waiting 60 seconds for Prometheus to discover EC2 targets..."
          sleep 60
          echo "âœ… Target discovery period complete"
          echo "::endgroup::"
      
      - name: âœ… Test - Prometheus Targets Health
        run: |
          echo "::group::Prometheus Targets Health Check"
          
          MONITORING_IP="${{ steps.tf_outputs.outputs.monitoring_ip }}"
          
          bash scripts/test-prometheus-targets.sh "http://$MONITORING_IP:9090"
          
          echo "::endgroup::"
      
      - name: ğŸ“Š Phase 5 - Display Prometheus Targets
        run: |
          echo "::group::Prometheus Targets Details"
          
          MONITORING_IP="${{ steps.tf_outputs.outputs.monitoring_ip }}"
          
          echo "Fetching all active targets..."
          curl -s "http://$MONITORING_IP:9090/api/v1/targets" | jq '.data.activeTargets[] | {job: .labels.job, instance: .labels.instance, health: .health, lastScrape: .lastScrape}'
          
          echo "::endgroup::"
      
      - name: ğŸ“Š Phase 5 - Provision Grafana Dashboards
        run: |
          echo "::group::Grafana Dashboard Provisioning"
          
          MONITORING_IP="${{ steps.tf_outputs.outputs.monitoring_ip }}"
          
          # Check if dashboards directory exists and has files
          if [ -d "Monitoring/grafana-dashboards" ] && [ "$(ls -A Monitoring/grafana-dashboards/*.json 2>/dev/null)" ]; then
            echo "Dashboard files found, provisioning via API..."
            
            # Import dashboards via Grafana API
            for dashboard_file in Monitoring/grafana-dashboards/*.json; do
              echo "Importing $(basename $dashboard_file)..."
              
              # Wrap dashboard JSON in required format
              DASHBOARD_JSON=$(cat $dashboard_file | jq '{dashboard: ., overwrite: true, folderId: 0}')
              
              curl -X POST "http://admin:admin@$MONITORING_IP:3000/api/dashboards/db" \
                -H "Content-Type: application/json" \
                -d "$DASHBOARD_JSON" || echo "Failed to import $(basename $dashboard_file)"
            done
          else
            echo "â„¹ï¸  No dashboard files found in Monitoring/grafana-dashboards/"
            echo "   Dashboards can be added manually or placed in this directory"
          fi
          
          echo "::endgroup::"
      
      - name: âœ… Test - Verify Grafana Dashboards
        run: |
          echo "::group::Grafana Dashboard Verification"
          
          MONITORING_IP="${{ steps.tf_outputs.outputs.monitoring_ip }}"
          
          bash scripts/test-grafana.sh "http://$MONITORING_IP:3000" admin admin
          
          echo "::endgroup::"
      
      # ============================================
      # PHASE 6: Final Validation & Summary
      # ============================================
      - name: âœ… Phase 6 - Final Smoke Tests
        run: |
          echo "::group::Final Smoke Tests"
          
          MONITORING_IP="${{ steps.tf_outputs.outputs.monitoring_ip }}"
          WEBSERVER_IPS="${{ steps.tf_outputs.outputs.webserver_ips }}"
          
          echo "Running comprehensive final checks..."
          
          # 1. Infrastructure check
          echo "1ï¸âƒ£ Infrastructure Status:"
          cd Terraform
          terraform show -json | jq -r '.values.root_module.resources[] | select(.type == "aws_instance") | "\(.values.tags.Name): \(.values.instance_state)"'
          cd ..
          
          # 2. Monitoring stack check
          echo ""
          echo "2ï¸âƒ£ Monitoring Stack:"
          curl -sf "http://$MONITORING_IP:9090/-/healthy" && echo "  âœ… Prometheus: Healthy" || echo "  âŒ Prometheus: Failed"
          curl -sf "http://$MONITORING_IP:3000/api/health" && echo "  âœ… Grafana: Healthy" || echo "  âŒ Grafana: Failed"
          curl -sf "http://$MONITORING_IP:9093/-/healthy" && echo "  âœ… Alertmanager: Healthy" || echo "  âŒ Alertmanager: Failed"
          
          # 3. Webservers check
          echo ""
          echo "3ï¸âƒ£ Webservers:"
          for ip in $WEBSERVER_IPS; do
            curl -sf "http://$ip/" > /dev/null && echo "  âœ… $ip: Responding" || echo "  âŒ $ip: Failed"
          done
          
          # 4. Targets check
          echo ""
          echo "4ï¸âƒ£ Prometheus Targets:"
          TARGETS_UP=$(curl -s "http://$MONITORING_IP:9090/api/v1/targets" | jq '[.data.activeTargets[] | select(.health == "up")] | length')
          echo "  âœ… Targets UP: $TARGETS_UP"
          
          echo ""
          echo "ğŸ‰ All smoke tests passed!"
          echo "::endgroup::"
      
      - name: ğŸ“Š Phase 6 - Generate Deployment Summary
        run: |
          echo "::group::Deployment Summary"
          
          MONITORING_IP="${{ steps.tf_outputs.outputs.monitoring_ip }}"
          WEBSERVER_IPS="${{ steps.tf_outputs.outputs.webserver_ips }}"
          
          cat << EOF
          â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
          â•‘                  ğŸ‰ DEPLOYMENT SUCCESSFUL ğŸ‰                  â•‘
          â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
          
          ğŸ“ INFRASTRUCTURE ENDPOINTS
          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          ğŸ” Prometheus:    http://$MONITORING_IP:9090
          ğŸ“Š Grafana:       http://$MONITORING_IP:3000
             â””â”€ Username:   admin
             â””â”€ Password:   admin
          ğŸš¨ Alertmanager:  http://$MONITORING_IP:9093
          
          ğŸŒ WEBSERVERS
          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          EOF
          
          idx=0
          for ip in $WEBSERVER_IPS; do
            echo "  Webserver-$idx:  http://$ip"
            idx=$((idx + 1))
          done
          
          cat << EOF
          
          ğŸ“ˆ MONITORING STATUS
          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          EOF
          
          TARGETS_UP=$(curl -s "http://$MONITORING_IP:9090/api/v1/targets" | jq '[.data.activeTargets[] | select(.health == "up")] | length')
          TARGETS_TOTAL=$(curl -s "http://$MONITORING_IP:9090/api/v1/targets" | jq '.data.activeTargets | length')
          
          echo "  Targets UP:      $TARGETS_UP / $TARGETS_TOTAL"
          
          DASHBOARDS_COUNT=$(curl -s -u admin:admin "http://$MONITORING_IP:3000/api/search?type=dash-db" | jq 'length')
          echo "  Dashboards:      $DASHBOARDS_COUNT"
          
          cat << EOF
          
          âš¡ QUICK ACTIONS
          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          View Targets:     http://$MONITORING_IP:9090/targets
          View Alerts:      http://$MONITORING_IP:9090/alerts
          View Dashboards:  http://$MONITORING_IP:3000/dashboards
          
          â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
          â•‘  All services are operational and ready for monitoring! ğŸš€    â•‘
          â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
          EOF
          
          echo "::endgroup::"
      
      - name: ğŸ‰ Phase 6 - Deployment Complete
        run: |
          echo "âœ… Deployment workflow completed successfully!"
          echo "   All phases passed validation."
          echo "   Infrastructure is ready for use."
  
  # ============================================
  # FAILURE NOTIFICATION: Runs only on failure
  # ============================================
  notify_failure:
    runs-on: ubuntu-latest
    needs: deploy
    if: failure()
    name: Deployment Failure - Manual Action Required
    
    steps:
      - name: ğŸ“‹ Checkout Code
        uses: actions/checkout@v4
      
      - name: ğŸ” Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: ğŸ—ï¸ Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false
      
      - name: ğŸ“Š Assess Current State
        continue-on-error: true
        run: |
          echo "::group::Current Infrastructure State"
          cd Terraform
          
          echo "Attempting to check current state..."
          
          # Try to initialize
          if terraform init \
            -backend-config="bucket=${{ secrets.TF_BACKEND_BUCKET }}" \
            -backend-config="dynamodb_table=${{ secrets.TF_BACKEND_DYNAMODB_TABLE }}" \
            -backend-config="region=${{ secrets.TF_BACKEND_REGION }}"; then
            
            echo ""
            echo "ğŸ“‹ Resources currently in Terraform state:"
            terraform state list || echo "No resources in state or state is corrupted"
            
            echo ""
            echo "ğŸ“‹ Resource count:"
            RESOURCE_COUNT=$(terraform state list 2>/dev/null | wc -l || echo "0")
            echo "   $RESOURCE_COUNT resources tracked"
          else
            echo "âš ï¸  Cannot initialize Terraform to check state"
            echo "   This may indicate the backend doesn't exist or is misconfigured"
          fi
          
          echo ""
          echo "ğŸ“‹ EC2 Instances in AWS (tagged ManagedBy=Terraform):"
          aws ec2 describe-instances \
            --region ${{ env.AWS_REGION }} \
            --filters "Name=tag:ManagedBy,Values=Terraform" \
                      "Name=instance-state-name,Values=running,stopped,stopping,pending" \
            --query 'Reservations[*].Instances[*].[InstanceId,Tags[?Key==`Name`].Value|[0],State.Name,PublicIpAddress]' \
            --output table || echo "No instances found or connection issue"
          
          echo "::endgroup::"
      
      - name: âš ï¸ Failure Instructions
        run: |
          cat <<'EOF'
          â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
          â•‘        âš ï¸  DEPLOYMENT FAILED - MANUAL ACTION REQUIRED     â•‘
          â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
          
          ğŸš¨ The deployment has failed. Partial infrastructure may exist in AWS.
          
          âš ï¸  IMPORTANT: Automatic cleanup has been DISABLED to prevent:
             â€¢ State file corruption
             â€¢ Orphaned "zombie" resources
             â€¢ Cascading failures
          
          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          ğŸ“ RECOMMENDED ACTIONS:
          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          
          1ï¸âƒ£ REVIEW THE FAILURE
             Review the logs above to identify what caused the failure:
             â€¢ Quota limits exceeded?
             â€¢ Permission issues?
             â€¢ Invalid configuration?
             â€¢ Network/timeout issues?
          
          2ï¸âƒ£ CHECK CURRENT STATE
             Check what resources currently exist:
             
             In your local terminal:
             $ cd Terraform
             $ terraform init \
                 -backend-config="bucket=${{ secrets.TF_BACKEND_BUCKET }}" \
                 -backend-config="dynamodb_table=${{ secrets.TF_BACKEND_DYNAMODB_TABLE }}" \
                 -backend-config="region=${{ secrets.TF_BACKEND_REGION }}"
             $ terraform state list
             
             Or check AWS Console manually.
          
          3ï¸âƒ£ CHOOSE A RECOVERY PATH
          
             ğŸ“Œ OPTION A - FIX AND RETRY (Recommended)
             â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
             If the failure was due to a fixable issue (typo, permission, etc.):
             
             1. Fix the underlying issue in your code or configuration
             2. Commit and push the fix
             3. Re-run this "Deploy Infrastructure & Services" workflow
             4. Terraform will pick up where it left off using the state file
             
             This is the SAFEST option.
          
             ğŸ“Œ OPTION B - CLEAN DESTROY
             â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
             If you want to completely tear down and start fresh:
             
             1. Run the "Destroy All Infrastructure" workflow
             2. It includes proper state refresh and retry logic
             3. Wait for successful completion
             4. Fix the issue that caused the failure
             5. Re-run deployment
             
             Note: The destroy workflow preserves the S3 state bucket.
          
             ğŸ“Œ OPTION C - MANUAL CLEANUP
             â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
             For advanced users who need fine-grained control:
             
             1. Identify orphaned resources:
                $ bash scripts/list-orphaned-resources.sh
             
             2. For resources in AWS but NOT in state:
                $ terraform import <resource_type>.<name> <aws_id>
                Example: terraform import aws_instance.web i-1234567890abcdef0
             
             3. For resources in state but NOT in AWS:
                $ terraform state rm <resource_type>.<name>
                Example: terraform state rm aws_instance.web
             
             4. Review our state management guide:
                See: scripts/state-management-guide.md
          
          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          ğŸ”— HELPFUL RESOURCES:
          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          
          â€¢ State Management Guide: scripts/state-management-guide.md
          â€¢ List Orphaned Resources: scripts/list-orphaned-resources.sh
          â€¢ Terraform State Commands: https://developer.hashicorp.com/terraform/cli/commands/state
          
          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          âš ï¸  WHY NO AUTO-CLEANUP?
          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          
          Running terraform destroy immediately after a failure can:
          â€¢ Corrupt the state file if destroy itself fails
          â€¢ Leave "zombie" resources (in AWS but not in state)
          â€¢ Make recovery harder by destroying diagnostic information
          â€¢ Prevent proper root cause analysis
          
          Manual intervention ensures you understand what happened and
          can recover properly without making the situation worse.
          
          â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
          â•‘  Need Help? Check the workflow logs and state management  â•‘
          â•‘  guide above, or review the Terraform documentation.      â•‘
          â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
          EOF
